{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590a051a",
   "metadata": {},
   "source": [
    "## Release notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e64c8",
   "metadata": {},
   "source": [
    "- v5\n",
    "- corrected optimizer\n",
    "- change inj softmax becase of instability \n",
    "changed\n",
    "def forward(self, x):\n",
    "        # Compute the max value for numerical stability\n",
    "        x_max = np.max(x, axis=1, keepdims=True)\n",
    "        \n",
    "        # Log-sum-exp trick for numerical stability in the softmax calculation\n",
    "        log_sum_exp = x_max + np.log(np.sum(np.exp(x - x_max), axis=1, keepdims=True))\n",
    "        \n",
    "        # Calculate the softmax output using the log-sum-exp trick\n",
    "        self.output = np.exp(x - log_sum_exp)\n",
    "        return self.output\n",
    "to\n",
    "def forward(self, x):\n",
    "        x_max = np.amax(x, axis=1, keepdims=True)\n",
    "        exp_x_shifted = np.exp(x - x_max)\n",
    "        self.output = exp_x_shifted / np.sum(exp_x_shifted, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "- v4 \n",
    "- Optimizer\n",
    "'''Update velocity using momentum and gradient\n",
    "velocity[i] = momentum * velocity[i] - lr * grad          \n",
    "Update parameter using new velocity\n",
    "param += velocity[i]\n",
    "Change to \n",
    "np.add(momentum * velocity[i], -lr * grad, out=velocity[i])\n",
    "- v3\n",
    "- add SGD with momentum\n",
    "- v2\n",
    "- Cleaned codes\n",
    "- vq\n",
    "-  Make all classes and compare them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0c12e",
   "metadata": {},
   "source": [
    "# Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2500151a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.061158Z",
     "start_time": "2023-11-15T15:07:28.404301Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b6fb6",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df9fa75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.064415Z",
     "start_time": "2023-11-15T15:07:29.062484Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a5bd4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.067366Z",
     "start_time": "2023-11-15T15:07:29.065014Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    batch = 256\n",
    "    shuffle = True\n",
    "    num_classes = 10\n",
    "    learning_rate = 0.01\n",
    "    epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be6713",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1feaf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.872849Z",
     "start_time": "2023-11-15T15:07:29.067939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset using NumPy\n",
    "# Load train data\n",
    "train_data = np.loadtxt('train.csv', delimiter=',', skiprows=1)\n",
    "train_labels = train_data[:, 0]  # Labels 0 and 1\n",
    "train_data = train_data[:, 1:]   # Pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b3898",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c4c0e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.875638Z",
     "start_time": "2023-11-15T15:07:29.873640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 7., 6., 9.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4182934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.884596Z",
     "start_time": "2023-11-15T15:07:29.876240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels) # from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110f9876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.887936Z",
     "start_time": "2023-11-15T15:07:29.885485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee60447",
   "metadata": {},
   "source": [
    "## Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e54e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.958355Z",
     "start_time": "2023-11-15T15:07:29.888902Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = np.reshape(train_data, (train_data.shape[0],28,28,1)).copy()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e718eca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:29.962043Z",
     "start_time": "2023-11-15T15:07:29.959563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc11b338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.029684Z",
     "start_time": "2023-11-15T15:07:29.963259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f01f98a1960>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1843b1ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.033007Z",
     "start_time": "2023-11-15T15:07:30.031502Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = np.einsum('bhwc ->bchw',train_data)\n",
    "\n",
    "train_data = train_data.reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50f4c2a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.036424Z",
     "start_time": "2023-11-15T15:07:30.033653Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, data, targets, batch_size, shuffle=True):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = len(self.data)\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for start_idx in range(0, self.num_samples, self.batch_size):\n",
    "            batch_indices = self.indices[start_idx:start_idx + self.batch_size]\n",
    "            batch_data = self.data[batch_indices]\n",
    "            batch_targets = self.targets[batch_indices]\n",
    "            yield batch_data, batch_targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e96ad079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.086725Z",
     "start_time": "2023-11-15T15:07:30.037007Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = CustomDataLoader(train_data[:10000].copy(), train_labels[:10000].copy(), CFG.batch, shuffle=CFG.shuffle)\n",
    "testloader = CustomDataLoader(train_data[10001:].copy(), train_labels[10001:].copy(), CFG.batch, shuffle=CFG.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce03b3c",
   "metadata": {},
   "source": [
    "# NN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62b6aa",
   "metadata": {},
   "source": [
    "## Linaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eace8543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.090227Z",
     "start_time": "2023-11-15T15:07:30.087539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Linear layer class\n",
    "# Define your custom Linear class\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.initialize_parameters()  # Initialize weights and biases\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.gradW = np.dot(self.input.T, grad_output)\n",
    "        self.gradB = np.sum(grad_output, axis=0)\n",
    "        return np.dot(grad_output, self.W.T)\n",
    "    def initialize_parameters(self):\n",
    "        # Initialize weights and biases using random normal distribution\n",
    "#         self.W = 0.1 *np.random.randn(self.in_features, self.out_features)\n",
    "#         self.W = np.random.randn(self.in_features, self.out_features) * np.sqrt(2. / (self.in_features + self.out_features))\n",
    "        # He Initialization\n",
    "        self.W = np.random.randn(self.in_features, self.out_features) * np.sqrt(2. / self.in_features)\n",
    "#         self.W = np.random.randn(self.in_features, self.out_features) * np.sqrt(1. / self.in_features)\n",
    "        self.b = np.random.randn(self.out_features)\n",
    "        # Initialize gradients to zero\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradB = np.zeros_like(self.b)\n",
    "    def reset_parameters(self):\n",
    "        # Call initialize_parameters to reset weights and biases\n",
    "        self.initialize_parameters()\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fa778",
   "metadata": {},
   "source": [
    "## Linear class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2616e340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.116195Z",
     "start_time": "2023-11-15T15:07:30.090864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Linear Forward Output:\n",
      "tensor([[-0.5294,  0.6013, -0.9705,  0.4259, -0.2862,  0.7784, -0.2301, -0.8150,\n",
      "          1.0823,  0.0883,  0.0182,  0.1063],\n",
      "        [-1.0140, -0.1944,  0.0846,  0.2452, -0.0832,  0.8907,  0.3326, -0.4187,\n",
      "          1.2698,  0.4541,  0.9577,  0.5667]], grad_fn=<AddmmBackward0>)\n",
      "Custom NumPy Linear Forward Output:\n",
      "[[ 0.73012173 -0.73856975  1.40818926  1.07306969  0.76011614  1.02937198\n",
      "   3.44996523 -0.5083243   1.53972151  1.30381235  0.12005495  1.82169921]\n",
      " [ 1.01504471 -1.13288727  0.49719983  2.99335261  0.47891938  0.56064231\n",
      "   1.28165034  0.55689401  0.6820143   0.92392894  2.66953722  2.38989491]]\n",
      "PyTorch Linear Gradient (Backward Output):\n",
      "<bound method Tensor.backward of tensor([[-0.5294,  0.6013, -0.9705,  0.4259, -0.2862,  0.7784, -0.2301, -0.8150,\n",
      "          1.0823,  0.0883,  0.0182,  0.1063],\n",
      "        [-1.0140, -0.1944,  0.0846,  0.2452, -0.0832,  0.8907,  0.3326, -0.4187,\n",
      "          1.2698,  0.4541,  0.9577,  0.5667]], grad_fn=<AddmmBackward0>)>\n",
      "Custom NumPy Linear Gradient (Backward Output):\n",
      "[[ 0.46962825 -0.99015848  0.34323288  0.35630203 -0.02768319 -0.76499156\n",
      "  -1.14288438  0.14217352  1.62751217 -0.44287509]\n",
      " [-0.51639145 -2.61680881  1.90913898 -0.27364209 -0.30678122 -1.14117429\n",
      "  -1.06138397  0.69883418  1.51860863 -1.31345286]]\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "input_data = torch.Tensor(np.random.randn(2, 10))\n",
    "fc_torch = nn.Linear(in_features=10, out_features=12, bias=True)\n",
    "fc_numpy = Linear(in_features=10, out_features=12)\n",
    "# Create a PyTorch Linear activation layer\n",
    "relu_torch = fc_torch\n",
    "\n",
    "# Create a custom Linear activation layer\n",
    "relu_numpy = fc_numpy\n",
    "\n",
    "# Forward pass through PyTorch Linear layer\n",
    "torch_out = relu_torch(input_data)\n",
    "\n",
    "# Forward pass through custom Linear layer\n",
    "numpy_out = relu_numpy(input_data.detach().numpy())  # Convert PyTorch tensor to NumPy array\n",
    "\n",
    "# # Backward pass through custom Linear layer (using the gradient output from the forward pass)\n",
    "grad_output = torch_out.detach().numpy()  # Convert PyTorch tensor to NumPy array\n",
    "custom_grad = relu_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch Linear Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy Linear Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "# # PyTorch's ReLU doesn't require backward pass for inference, so there's no gradient to print for PyTorch.\n",
    "print(\"PyTorch Linear Gradient (Backward Output):\")\n",
    "print(torch_out.backward)\n",
    "print(\"Custom NumPy Linear Gradient (Backward Output):\")\n",
    "print(custom_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91742288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.146369Z",
     "start_time": "2023-11-15T15:07:30.117054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Forward Output:\n",
      "tensor([[-0.2885, -0.4261, -0.6650, -0.5358,  0.1469, -0.0056, -0.0767, -0.1290,\n",
      "         -0.4300,  0.0019, -0.1329,  0.7975],\n",
      "        [ 0.1633, -0.7298, -0.4296, -0.0427, -0.2498,  0.3202,  0.0497,  0.9181,\n",
      "         -0.2239,  0.4121, -0.3931,  0.8129]], grad_fn=<AddmmBackward0>)\n",
      "Custom NumPy Forward Output:\n",
      "[[-3.5942627  -2.65368792 -0.07871125 -3.58333021 -2.20965156  2.63899389\n",
      "  -0.09572264  1.60343825  0.88077649 -3.86024856 -1.68469771  0.0680101 ]\n",
      " [-1.44368016 -0.74989252  0.47696987 -2.96897747 -0.17594518  2.15836321\n",
      "  -1.66366363  1.09110309  2.02909923 -1.3178046  -1.30130285 -1.56902875]]\n",
      "PyTorch Weight Gradient:\n",
      "tensor([[-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874]])\n",
      "Custom NumPy Weight Gradient:\n",
      "[[-1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882\n",
      "  -1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882]\n",
      " [-1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578\n",
      "  -1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578]\n",
      " [-0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215\n",
      "  -0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215 ]\n",
      " [-2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539\n",
      "  -2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539]\n",
      " [ 1.59458314  1.59458314  1.59458314  1.59458314  1.59458314  1.59458314\n",
      "   1.59458314  1.59458314  1.59458314  1.59458314  1.59458314  1.59458314]\n",
      " [-1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259\n",
      "  -1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259]\n",
      " [ 2.13842528  2.13842528  2.13842528  2.13842528  2.13842528  2.13842528\n",
      "   2.13842528  2.13842528  2.13842528  2.13842528  2.13842528  2.13842528]\n",
      " [ 1.47762419  1.47762419  1.47762419  1.47762419  1.47762419  1.47762419\n",
      "   1.47762419  1.47762419  1.47762419  1.47762419  1.47762419  1.47762419]\n",
      " [ 0.63552969  0.63552969  0.63552969  0.63552969  0.63552969  0.63552969\n",
      "   0.63552969  0.63552969  0.63552969  0.63552969  0.63552969  0.63552969]\n",
      " [-1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718\n",
      "  -1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718]]\n",
      "PyTorch Bias Gradient:\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Custom NumPy Bias Gradient:\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Create PyTorch and custom Linear layers\n",
    "fc_torch = nn.Linear(in_features=10, out_features=12, bias=True)\n",
    "fc_numpy = Linear(in_features=10, out_features=12)\n",
    "\n",
    "# Generate random input data\n",
    "input_data = np.random.randn(2, 10)\n",
    "\n",
    "# Forward pass through PyTorch layer\n",
    "torch_out = fc_torch(torch.Tensor(input_data))\n",
    "\n",
    "# Backward pass through PyTorch layer (this computes gradients)\n",
    "torch_out.backward(torch.ones_like(torch_out))\n",
    "\n",
    "# Forward pass through your custom Linear layer\n",
    "numpy_out = fc_numpy(input_data)\n",
    "\n",
    "# Backward pass through your custom Linear layer (using the gradient output from the forward pass)\n",
    "grad_output = np.ones_like(numpy_out)  # Gradient for the custom layer\n",
    "custom_grad = fc_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "print(\"PyTorch Weight Gradient:\")\n",
    "print(fc_torch.weight.grad)\n",
    "\n",
    "print(\"Custom NumPy Weight Gradient:\")\n",
    "print(fc_numpy.gradW)\n",
    "\n",
    "print(\"PyTorch Bias Gradient:\")\n",
    "print(fc_torch.bias.grad)\n",
    "\n",
    "print(\"Custom NumPy Bias Gradient:\")\n",
    "print(fc_numpy.gradB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71fcc8",
   "metadata": {},
   "source": [
    "## Relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8eb2e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.149415Z",
     "start_time": "2023-11-15T15:07:30.147395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define ReLU activation class\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x  # Store input for backpropagation\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input > 0)\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64ad04",
   "metadata": {},
   "source": [
    "## Relu class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0d9f350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.154331Z",
     "start_time": "2023-11-15T15:07:30.150047Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ReLU Forward Output:\n",
      "tensor([[0.0000, 0.8030, 0.0000, 0.6030, 0.7941, 0.0000, 0.0000, 0.1388, 1.1424,\n",
      "         0.0000],\n",
      "        [1.1378, 0.2551, 0.0000, 0.0000, 0.0000, 0.0000, 0.6068, 0.7978, 0.0000,\n",
      "         0.0477]])\n",
      "Custom NumPy ReLU Forward Output:\n",
      "[[0.         0.8029657  0.         0.6029578  0.7940915  0.\n",
      "  0.         0.13880739 1.1424047  0.        ]\n",
      " [1.1377505  0.25512746 0.         0.         0.         0.\n",
      "  0.60684925 0.79776895 0.         0.04768137]]\n",
      "PyTorch ReLU Gradient (Backward Output):\n",
      "<bound method Tensor.backward of tensor([[0.0000, 0.8030, 0.0000, 0.6030, 0.7941, 0.0000, 0.0000, 0.1388, 1.1424,\n",
      "         0.0000],\n",
      "        [1.1378, 0.2551, 0.0000, 0.0000, 0.0000, 0.0000, 0.6068, 0.7978, 0.0000,\n",
      "         0.0477]])>\n",
      "Custom NumPy ReLU Gradient (Backward Output):\n",
      "[[0.         0.8029657  0.         0.6029578  0.7940915  0.\n",
      "  0.         0.13880739 1.1424047  0.        ]\n",
      " [1.1377505  0.25512746 0.         0.         0.         0.\n",
      "  0.60684925 0.79776895 0.         0.04768137]]\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "input_data = torch.Tensor(np.random.randn(2, 10))\n",
    "\n",
    "# Create a PyTorch ReLU activation layer\n",
    "relu_torch = F.relu\n",
    "\n",
    "# Create a custom ReLU activation layer\n",
    "relu_numpy = ReLU()\n",
    "\n",
    "# Forward pass through PyTorch ReLU layer\n",
    "torch_out = relu_torch(input_data)\n",
    "\n",
    "# Forward pass through custom ReLU layer\n",
    "numpy_out = relu_numpy(input_data.detach().numpy())  # Convert PyTorch tensor to NumPy array\n",
    "\n",
    "# # Backward pass through custom ReLU layer (using the gradient output from the forward pass)\n",
    "grad_output = torch_out.detach().numpy()  # Convert PyTorch tensor to NumPy array\n",
    "custom_grad = relu_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch ReLU Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy ReLU Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "print(\"PyTorch ReLU Gradient (Backward Output):\")\n",
    "print(torch_out.backward)\n",
    "print(\"Custom NumPy ReLU Gradient (Backward Output):\")\n",
    "print(custom_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67191df",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bdb4b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.156641Z",
     "start_time": "2023-11-15T15:07:30.155023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-07"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d667a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.159796Z",
     "start_time": "2023-11-15T15:07:30.157298Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Compute the max value for numerical stability\n",
    "#         x_max = np.max(x, axis=1, keepdims=True)\n",
    "        \n",
    "#         # Log-sum-exp trick for numerical stability in the softmax calculation\n",
    "#         log_sum_exp = x_max + np.log(np.sum(np.exp(x - x_max), axis=1, keepdims=True))+0.000000001\n",
    "        \n",
    "#         # Calculate the softmax output using the log-sum-exp trick\n",
    "#         self.output = np.exp(x - log_sum_exp)\n",
    "#         return self.output\n",
    "    def forward(self, x):\n",
    "        # Compute the max value for numerical stability\n",
    "        x_max = np.maximum(0,x)+1e-15\n",
    "        \n",
    "        # Log-sum-exp trick for numerical stability in the softmax calculation\n",
    "        log_sum_exp = x_max + np.log(np.sum(np.exp(x - x_max)+1e-15, axis=1, keepdims=True))\n",
    "        \n",
    "        # Calculate the softmax output using the log-sum-exp trick\n",
    "        self.output = np.exp(x - log_sum_exp)\n",
    "        return self.output\n",
    "    def backward(self, d_output):\n",
    "        # Using broadcasting and vectorization to compute d_input in one shot\n",
    "        output_expanded = np.expand_dims(self.output, axis=2)  # Expand output dims for broadcasting\n",
    "        d_output_expanded = np.expand_dims(d_output, axis=2)  # Expand d_output dims for broadcasting\n",
    "        \n",
    "        jacobian_matrix = np.eye(self.output.shape[1]) - output_expanded * np.swapaxes(output_expanded, 1, 2)\n",
    "\n",
    "        d_input = np.einsum('ijk,ikj->ik', jacobian_matrix, d_output_expanded).squeeze()\n",
    "\n",
    "        return d_input\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17f600d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.163734Z",
     "start_time": "2023-11-15T15:07:30.160367Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "    def forward(self, x):\n",
    "#         if np.isnan(x).any() or np.isinf(x).any():\n",
    "#             print(\"Warning: Input contains NaN or Inf values.\")\n",
    "#             return None\n",
    "        # Compute the max value for numerical stability\n",
    "        x_max = np.amax(x, axis=1, keepdims=True)\n",
    "        x_sub = np.subtract(x,x_max)\n",
    "        # Log-sum-exp trick for numerical stability in the softmax calculation\n",
    "        log_sum_exp = x_max + np.log(np.sum(np.exp(x_sub), axis=1, keepdims=True))\n",
    "        \n",
    "        # Calculate the softmax output using the log-sum-exp trick\n",
    "        self.output = np.exp(x - log_sum_exp)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Using broadcasting and vectorization to compute d_input in one shot\n",
    "        output_expanded = np.expand_dims(self.output, axis=2)  # Expand output dims for broadcasting\n",
    "        d_output_expanded = np.expand_dims(d_output, axis=2)  # Expand d_output dims for broadcasting\n",
    "        \n",
    "        jacobian_matrix = np.eye(self.output.shape[1]) - output_expanded * np.swapaxes(output_expanded, 1, 2)\n",
    "\n",
    "        d_input = np.einsum('ijk,ikj->ik', jacobian_matrix, d_output_expanded).squeeze()\n",
    "\n",
    "        return d_input\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91fb73",
   "metadata": {},
   "source": [
    "## Softmax class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "358051fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.168712Z",
     "start_time": "2023-11-15T15:07:30.164319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Softmax Forward Output:\n",
      "tensor([[0.0330, 0.0696, 0.4259, 0.1612, 0.0544, 0.0499, 0.0403, 0.0143, 0.0362,\n",
      "         0.1152],\n",
      "        [0.0346, 0.0663, 0.0287, 0.1926, 0.0246, 0.1588, 0.2456, 0.0635, 0.0442,\n",
      "         0.1412]])\n",
      "Custom NumPy Softmax Forward Output:\n",
      "[[0.03298178 0.06958842 0.425897   0.16116287 0.05441616 0.04987547\n",
      "  0.04029905 0.01431992 0.03622704 0.11523236]\n",
      " [0.0345681  0.06634659 0.02869631 0.19255741 0.02457472 0.15879716\n",
      "  0.24557205 0.06353392 0.04416914 0.14118452]]\n",
      "PyTorch Softmax Gradient (Backward Output):\n",
      "<bound method Tensor.backward of tensor([[0.0330, 0.0696, 0.4259, 0.1612, 0.0544, 0.0499, 0.0403, 0.0143, 0.0362,\n",
      "         0.1152],\n",
      "        [0.0346, 0.0663, 0.0287, 0.1926, 0.0246, 0.1588, 0.2456, 0.0635, 0.0442,\n",
      "         0.1412]])>\n",
      "Custom NumPy Softmax Gradient (Backward Output):\n",
      "[[0.03189397 0.06474587 0.24450872 0.13518938 0.05145505 0.04738791\n",
      "  0.03867503 0.01411486 0.03491464 0.10195384]\n",
      " [0.03337315 0.06194473 0.02787284 0.15547907 0.0239708  0.13358064\n",
      "  0.18526643 0.05949737 0.04221823 0.12125146]]\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "input_data = torch.Tensor(np.random.randn(2, 10))\n",
    "\n",
    "# Create a PyTorch Softmax activation layer\n",
    "softmax_torch = F.softmax\n",
    "\n",
    "# Create a custom Softmax activation layer\n",
    "softmax_numpy = Softmax()\n",
    "\n",
    "# Forward pass through PyTorch Softmax layer\n",
    "torch_out = softmax_torch(input_data, dim=1)\n",
    "\n",
    "# Forward pass through custom Softmax layer\n",
    "numpy_out = softmax_numpy(input_data.detach().numpy())  # Convert PyTorch tensor to NumPy array\n",
    "\n",
    "# # Backward pass through custom Softmax layer (using the gradient output from the forward pass)\n",
    "grad_output = torch_out.detach().numpy()  # Convert PyTorch tensor to NumPy array\n",
    "custom_grad = softmax_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch Softmax Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy Softmax Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "print(\"PyTorch Softmax Gradient (Backward Output):\")\n",
    "print(torch_out.backward)\n",
    "print(\"Custom NumPy Softmax Gradient (Backward Output):\")\n",
    "print(custom_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22d009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:38:36.364408Z",
     "start_time": "2023-09-20T04:38:36.362855Z"
    }
   },
   "source": [
    "## CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d44396cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.171461Z",
     "start_time": "2023-11-15T15:07:30.169304Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.softmax = Softmax()\n",
    "        self.epsilon = 1e-15\n",
    "    def forward(self, pred, y):\n",
    "        n = y.shape[0]\n",
    "        pred_softmax = self.softmax(pred)\n",
    "        log_pred = -np.log(pred_softmax[np.arange(n), y] + self.epsilon)\n",
    "        loss = np.sum(log_pred) / n\n",
    "\n",
    "        self.cache['pred_softmax'] = pred_softmax\n",
    "        self.cache['y'] = y\n",
    "        self.cache['n'] = n\n",
    "        self.cache['loss'] = loss\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        y = self.cache['y']\n",
    "        n = self.cache['n']\n",
    "        loss = self.cache['loss']\n",
    "        y = np.eye(CFG.num_classes)[y]\n",
    "        grad_input = self.softmax.output - y\n",
    "        grad_input /=n\n",
    "        return grad_input\n",
    "    \n",
    "    def __call__(self, pred, y):\n",
    "        return self.forward(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1111e",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c67ddbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.176428Z",
     "start_time": "2023-11-15T15:07:30.172036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients with respect to input predictions:\n",
      "tensor([[ 0.0856,  0.0660,  0.0160, -0.2429,  0.0137,  0.0032,  0.0099,  0.0271,\n",
      "          0.0119,  0.0095],\n",
      "        [ 0.0186,  0.0151,  0.0420,  0.0052,  0.0334,  0.0188, -0.2196,  0.0049,\n",
      "          0.0477,  0.0339],\n",
      "        [ 0.0886,  0.0072,  0.0093,  0.0028,  0.0073, -0.1802,  0.0223,  0.0080,\n",
      "          0.0190,  0.0156],\n",
      "        [ 0.0031, -0.2463,  0.0635,  0.0439,  0.0048,  0.0185,  0.0103,  0.0624,\n",
      "          0.0329,  0.0070]])\n",
      "Gradients with respect to input predictions:\n",
      "[[ 0.08563988  0.06598342  0.01602204 -0.24286835  0.01367588  0.00318298\n",
      "   0.00989491  0.02706582  0.01194149  0.00946193]\n",
      " [ 0.01864543  0.01505539  0.04195438  0.00522959  0.03336666  0.01882059\n",
      "  -0.21957186  0.00491351  0.04766207  0.03392425]\n",
      " [ 0.08861818  0.00723111  0.00926416  0.00279253  0.0072975  -0.18021398\n",
      "   0.02233584  0.00804276  0.01899586  0.01563605]\n",
      " [ 0.00311305 -0.24626936  0.06352987  0.04387507  0.0047598   0.01850181\n",
      "   0.01026798  0.0624005   0.0328616   0.00695966]]\n"
     ]
    }
   ],
   "source": [
    "pred = np.random.randn(4, 10)\n",
    "y = np.random.randint(0, 10, (4,))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize random predictions (logits) and labels for demonstration\n",
    "# Assume 4 samples and 10 classes\n",
    "\n",
    "pred_torch = torch.Tensor(pred.copy())\n",
    "pred_torch.requires_grad_()\n",
    "y_torch = torch.Tensor(y.copy()).long()\n",
    "# Initialize loss function\n",
    "loss_fn_torch = nn.CrossEntropyLoss()\n",
    "loss_fn_numpy = CrossEntropyLoss()\n",
    "# Forward pass to compute the loss\n",
    "loss_torch = loss_fn_torch(pred_torch, y_torch)\n",
    "loss_numpy = loss_fn_numpy(pred, y)\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss_torch.backward()\n",
    "\n",
    "# Print computed gradients for the input predictions\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(pred_torch.grad)\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(loss_fn_numpy.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c188ae6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.179781Z",
     "start_time": "2023-11-15T15:07:30.177697Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn_torch = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e8176a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.183140Z",
     "start_time": "2023-11-15T15:07:30.180903Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = np.random.randn(128,10)\n",
    "y = np.random.randint(0, 10, (128,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baceed0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.189524Z",
     "start_time": "2023-11-15T15:07:30.184128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients with respect to input predictions:\n",
      "tensor([[ 0.0012,  0.0010,  0.0010,  ...,  0.0003, -0.0075,  0.0016],\n",
      "        [ 0.0007,  0.0020,  0.0004,  ...,  0.0008,  0.0005,  0.0003],\n",
      "        [ 0.0007,  0.0020,  0.0006,  ...,  0.0002,  0.0005,  0.0018],\n",
      "        ...,\n",
      "        [ 0.0009,  0.0008,  0.0007,  ...,  0.0018,  0.0005,  0.0008],\n",
      "        [ 0.0003,  0.0013,  0.0016,  ...,  0.0013,  0.0005, -0.0063],\n",
      "        [ 0.0002,  0.0005,  0.0005,  ...,  0.0016,  0.0012,  0.0004]])\n",
      "Gradients with respect to input predictions:\n",
      "[[ 0.00116381  0.00099039  0.00099311 ...  0.00029668 -0.00745106\n",
      "   0.00157419]\n",
      " [ 0.00072999  0.00200638  0.00037059 ...  0.00083348  0.00051905\n",
      "   0.00025214]\n",
      " [ 0.00068369  0.00203043  0.00061834 ...  0.0001946   0.00045953\n",
      "   0.00180476]\n",
      " ...\n",
      " [ 0.00087967  0.00078998  0.00073329 ...  0.00177626  0.00051655\n",
      "   0.00084896]\n",
      " [ 0.00027092  0.00132709  0.0016218  ...  0.00134401  0.00045288\n",
      "  -0.00629786]\n",
      " [ 0.00023766  0.0004959   0.00052891 ...  0.00162981  0.001194\n",
      "   0.00043709]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pred_torch = torch.Tensor(pred.copy())\n",
    "pred_torch.requires_grad_()\n",
    "y_torch = torch.Tensor(y.copy()).long()\n",
    "# Initialize loss function\n",
    "loss_fn_torch = nn.CrossEntropyLoss()\n",
    "loss_fn_numpy = CrossEntropyLoss()\n",
    "# Forward pass to compute the loss\n",
    "loss_torch = loss_fn_torch(pred_torch, y_torch)\n",
    "loss_numpy = loss_fn_numpy(pred, y)\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss_torch.backward()\n",
    "\n",
    "# Print computed gradients for the input predictions\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(pred_torch.grad)\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(loss_fn_numpy.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fd028",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87842120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.193525Z",
     "start_time": "2023-11-15T15:07:30.190309Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_size, out_dim):\n",
    "        self.fc1 = Linear(in_features = input_size, out_features = 32)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(in_features=32, out_features=64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc3 = Linear(in_features=64, out_features=out_dim)\n",
    "        self.relu3 = ReLU()\n",
    "        self.soft = Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "#         x = \n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.soft(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output = self.soft.backward(grad_output)\n",
    "        grad_output = self.relu3.backward(grad_output)\n",
    "        grad_output = self.fc3.backward(grad_output)\n",
    "        grad_output = self.relu2.backward(grad_output)\n",
    "        grad_output = self.fc2.backward(grad_output)\n",
    "        grad_output = self.relu1.backward(grad_output)\n",
    "        grad_output = self.fc1.backward(grad_output)\n",
    "        \n",
    "    def params(self):\n",
    "        # Collect parameters and their gradients in a list of tuples\n",
    "        parameters = [\n",
    "            (self.fc1.W, self.fc1.gradW),\n",
    "            (self.fc1.b, self.fc1.gradB),\n",
    "            (self.fc2.W, self.fc2.gradW),\n",
    "            (self.fc2.b, self.fc2.gradB),\n",
    "            (self.fc3.W, self.fc3.gradW),\n",
    "            (self.fc3.b, self.fc3.gradB)\n",
    "        ]\n",
    "        return parameters\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dda7895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.197378Z",
     "start_time": "2023-11-15T15:07:30.194324Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SimpleNN(28*28,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66af13",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "291f6d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.200555Z",
     "start_time": "2023-11-15T15:07:30.198143Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    @classmethod\n",
    "    def step(cls, parameters, lr, momentum = 0.9,velocity=None, with_momentum = False):\n",
    "        if with_momentum and velocity is None:\n",
    "            # Init velocity\n",
    "            velocity = [np.zeros_like(param) for param, _ in parameters]\n",
    "            \n",
    "        if with_momentum:\n",
    "            for i, (param, grad) in enumerate(parameters):\n",
    "                # Update velocity using momentum and gradient\n",
    "                \n",
    "                np.add(momentum * velocity[i], -lr * grad, out=velocity[i])\n",
    "                # Update the parameter\n",
    "                np.add(param,velocity[i], out=param)\n",
    "        else: \n",
    "            for param, grad in parameters:\n",
    "                np.subtract(param, lr * grad, out = param)\n",
    "                \n",
    "    @classmethod        \n",
    "    def zero_grad(cls, parameters):\n",
    "        for _, grad in parameters:\n",
    "            grad.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a29e4aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.204175Z",
     "start_time": "2023-11-15T15:07:30.201258Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77800a9c",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61e04c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.207119Z",
     "start_time": "2023-11-15T15:07:30.204815Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa48ee5",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21c4158a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.210278Z",
     "start_time": "2023-11-15T15:07:30.207668Z"
    }
   },
   "outputs": [],
   "source": [
    "class Clip_gradients:\n",
    "    @classmethod \n",
    "    def clip(cls, parameters, max_value):\n",
    "        # Assume gradients is a list of gradients (one per layer)\n",
    "        gradients = [param[1] for param in parameters]\n",
    "        for gradient in gradients:\n",
    "            np.clip(gradient, -max_value, max_value, out=gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eec09f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.213491Z",
     "start_time": "2023-11-15T15:07:30.210831Z"
    }
   },
   "outputs": [],
   "source": [
    "class Clip_grad:\n",
    "    def __init__(self, model, max_value):\n",
    "        self.model = model\n",
    "        self.max_value = max_value\n",
    "    def clip(self):\n",
    "        gradients = [param[1] for param in self.model.params()]\n",
    "        for gradient in gradients:\n",
    "            np.clip(gradient, -self.max_value, self.max_value, out=gradient)\n",
    "    def __call__(self):\n",
    "        return self.clip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "deb18452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.216578Z",
     "start_time": "2023-11-15T15:07:30.214078Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# def clip_gradients(gradients, max_value):\n",
    "#     # Assume gradients is a list of gradients (one per layer)\n",
    "#     for gradient in gradients:\n",
    "#         np.clip(gradient, -max_value, max_value, out=gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41d802a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.219537Z",
     "start_time": "2023-11-15T15:07:30.217128Z"
    }
   },
   "outputs": [],
   "source": [
    "# clip = Clip_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be386543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.222569Z",
     "start_time": "2023-11-15T15:07:30.220092Z"
    }
   },
   "outputs": [],
   "source": [
    "clip_gradients = Clip_grad(model, max_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dccc7c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.226548Z",
     "start_time": "2023-11-15T15:07:30.224240Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader)\n",
    "    num_batches =(len(dataloader) + CFG.batch - 1) // CFG.batch\n",
    "    \n",
    "    train_loss, correct = 0, 0\n",
    "    \n",
    "    for b, (X, y) in enumerate(dataloader):\n",
    "#         y = y.astype(int)\n",
    "        y = y.astype(np.int32)\n",
    "        pred = model(X)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "        grad = loss_fn.backward()\n",
    "#         # Clip gradients here\n",
    "#         gradients = [param[1] for param in model.params()]  # Extracting grad from (param, grad)\n",
    "#         clip_gradients(gradients, max_value=5)  # max_value is a hyperparameter\n",
    "#         clip.clip(model.params(), max_value=5)\n",
    "        clip_gradients()\n",
    "        model.backward(grad)\n",
    "        # Update weights\n",
    "        optimizer.step(model.params(), CFG.learning_rate,momentum=0.9, with_momentum=True)  # To update parameters\n",
    "        optimizer.zero_grad(model.params())  # To reset gradients to zero\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum()       \n",
    "    train_loss /= num_batches\n",
    "    accuracy = (100 * correct) / size\n",
    "    print(f\"Train error:\\n Accuracy: {accuracy:.1f}%  Avg loss: {train_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8043e00",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f84b3a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:30.230518Z",
     "start_time": "2023-11-15T15:07:30.227122Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for b, (X, y) in enumerate(testloader):\n",
    "        y = y.astype(int)\n",
    "#         y = y.astype(np.int32)\n",
    "#         y = np.eye(CFG.num_classes)[y]\n",
    "        pred = model(X)\n",
    "#         pred = pred.astype(int)\n",
    "#         pred = np.eye(CFG.num_classes)[pred]\n",
    "        test_loss += loss_fn(pred, y)\n",
    "        total += len(y)\n",
    "        correct += (pred.argmax(1) == y).sum()\n",
    "    loss = test_loss / b\n",
    "    correct = 100.*correct/total\n",
    "    print(f\"Test Error:\\n Accuracy: {(correct):>.1f}%, Avg loss: {(loss):>.8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415884e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9484c202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:07:52.484108Z",
     "start_time": "2023-11-15T15:07:30.231092Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 10.1%  Avg loss: 2.30750621\n",
      "Test Error:\n",
      " Accuracy: 10.5%, Avg loss: 2.30519781\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 14.2%  Avg loss: 2.26614866\n",
      "Test Error:\n",
      " Accuracy: 18.8%, Avg loss: 2.26570383\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 35.6%  Avg loss: 2.22501250\n",
      "Test Error:\n",
      " Accuracy: 43.7%, Avg loss: 2.22335343\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 49.1%  Avg loss: 2.18257318\n",
      "Test Error:\n",
      " Accuracy: 51.7%, Avg loss: 2.18131459\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 54.1%  Avg loss: 2.14152694\n",
      "Test Error:\n",
      " Accuracy: 55.1%, Avg loss: 2.14128983\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 56.8%  Avg loss: 2.10216841\n",
      "Test Error:\n",
      " Accuracy: 57.2%, Avg loss: 2.10383817\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 58.6%  Avg loss: 2.06581004\n",
      "Test Error:\n",
      " Accuracy: 58.4%, Avg loss: 2.07035889\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 59.8%  Avg loss: 2.03394893\n",
      "Test Error:\n",
      " Accuracy: 59.3%, Avg loss: 2.04116109\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 60.5%  Avg loss: 2.00581711\n",
      "Test Error:\n",
      " Accuracy: 60.1%, Avg loss: 2.01455192\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 62.1%  Avg loss: 1.98044051\n",
      "Test Error:\n",
      " Accuracy: 62.2%, Avg loss: 1.99076826\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 63.8%  Avg loss: 1.95748926\n",
      "Test Error:\n",
      " Accuracy: 63.5%, Avg loss: 1.96933283\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 64.9%  Avg loss: 1.93733421\n",
      "Test Error:\n",
      " Accuracy: 64.6%, Avg loss: 1.95091579\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 65.7%  Avg loss: 1.92017538\n",
      "Test Error:\n",
      " Accuracy: 65.3%, Avg loss: 1.93509279\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 66.2%  Avg loss: 1.90530066\n",
      "Test Error:\n",
      " Accuracy: 65.9%, Avg loss: 1.92142009\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 66.7%  Avg loss: 1.89234597\n",
      "Test Error:\n",
      " Accuracy: 66.4%, Avg loss: 1.90936215\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 67.1%  Avg loss: 1.88091307\n",
      "Test Error:\n",
      " Accuracy: 66.9%, Avg loss: 1.89860717\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 67.6%  Avg loss: 1.87081094\n",
      "Test Error:\n",
      " Accuracy: 67.3%, Avg loss: 1.88909328\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 67.8%  Avg loss: 1.86184526\n",
      "Test Error:\n",
      " Accuracy: 67.7%, Avg loss: 1.88084049\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 68.1%  Avg loss: 1.85385226\n",
      "Test Error:\n",
      " Accuracy: 68.0%, Avg loss: 1.87321878\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 68.5%  Avg loss: 1.84656794\n",
      "Test Error:\n",
      " Accuracy: 68.3%, Avg loss: 1.86640457\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 68.7%  Avg loss: 1.84000938\n",
      "Test Error:\n",
      " Accuracy: 68.6%, Avg loss: 1.86033717\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 68.9%  Avg loss: 1.83416043\n",
      "Test Error:\n",
      " Accuracy: 68.9%, Avg loss: 1.85469935\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 69.2%  Avg loss: 1.82870745\n",
      "Test Error:\n",
      " Accuracy: 69.1%, Avg loss: 1.84956046\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 69.5%  Avg loss: 1.82372685\n",
      "Test Error:\n",
      " Accuracy: 69.3%, Avg loss: 1.84460099\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 69.8%  Avg loss: 1.81904092\n",
      "Test Error:\n",
      " Accuracy: 69.6%, Avg loss: 1.84024239\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 70.1%  Avg loss: 1.81481263\n",
      "Test Error:\n",
      " Accuracy: 69.8%, Avg loss: 1.83613747\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 70.3%  Avg loss: 1.81089830\n",
      "Test Error:\n",
      " Accuracy: 70.1%, Avg loss: 1.83245273\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 70.5%  Avg loss: 1.80726063\n",
      "Test Error:\n",
      " Accuracy: 70.2%, Avg loss: 1.82900161\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 70.7%  Avg loss: 1.80383905\n",
      "Test Error:\n",
      " Accuracy: 70.4%, Avg loss: 1.82569328\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 70.9%  Avg loss: 1.80056146\n",
      "Test Error:\n",
      " Accuracy: 70.6%, Avg loss: 1.82259882\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 71.1%  Avg loss: 1.79756381\n",
      "Test Error:\n",
      " Accuracy: 70.7%, Avg loss: 1.81972866\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 71.3%  Avg loss: 1.79475750\n",
      "Test Error:\n",
      " Accuracy: 71.0%, Avg loss: 1.81695449\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 71.4%  Avg loss: 1.79211553\n",
      "Test Error:\n",
      " Accuracy: 71.1%, Avg loss: 1.81439457\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 71.6%  Avg loss: 1.78959561\n",
      "Test Error:\n",
      " Accuracy: 71.3%, Avg loss: 1.81198882\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 71.8%  Avg loss: 1.78722408\n",
      "Test Error:\n",
      " Accuracy: 71.5%, Avg loss: 1.80963523\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 72.1%  Avg loss: 1.78491590\n",
      "Test Error:\n",
      " Accuracy: 71.7%, Avg loss: 1.80743944\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 72.2%  Avg loss: 1.78276261\n",
      "Test Error:\n",
      " Accuracy: 71.9%, Avg loss: 1.80536490\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 72.4%  Avg loss: 1.78070804\n",
      "Test Error:\n",
      " Accuracy: 72.0%, Avg loss: 1.80340141\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 72.5%  Avg loss: 1.77879183\n",
      "Test Error:\n",
      " Accuracy: 72.2%, Avg loss: 1.80159585\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 72.8%  Avg loss: 1.77697355\n",
      "Test Error:\n",
      " Accuracy: 72.3%, Avg loss: 1.79983370\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 73.0%  Avg loss: 1.77527126\n",
      "Test Error:\n",
      " Accuracy: 72.5%, Avg loss: 1.79820039\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 73.2%  Avg loss: 1.77364568\n",
      "Test Error:\n",
      " Accuracy: 72.6%, Avg loss: 1.79660106\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 73.3%  Avg loss: 1.77205598\n",
      "Test Error:\n",
      " Accuracy: 72.7%, Avg loss: 1.79507732\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 73.5%  Avg loss: 1.77049824\n",
      "Test Error:\n",
      " Accuracy: 72.9%, Avg loss: 1.79350113\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 73.6%  Avg loss: 1.76895882\n",
      "Test Error:\n",
      " Accuracy: 73.0%, Avg loss: 1.79202287\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 73.8%  Avg loss: 1.76749034\n",
      "Test Error:\n",
      " Accuracy: 73.1%, Avg loss: 1.79066690\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 74.0%  Avg loss: 1.76612142\n",
      "Test Error:\n",
      " Accuracy: 73.2%, Avg loss: 1.78933728\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 74.2%  Avg loss: 1.76473491\n",
      "Test Error:\n",
      " Accuracy: 73.4%, Avg loss: 1.78799420\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 74.3%  Avg loss: 1.76339705\n",
      "Test Error:\n",
      " Accuracy: 73.5%, Avg loss: 1.78664628\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 74.5%  Avg loss: 1.76206767\n",
      "Test Error:\n",
      " Accuracy: 73.6%, Avg loss: 1.78530314\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 74.8%  Avg loss: 1.76079831\n",
      "Test Error:\n",
      " Accuracy: 73.7%, Avg loss: 1.78415320\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 74.9%  Avg loss: 1.75958360\n",
      "Test Error:\n",
      " Accuracy: 73.8%, Avg loss: 1.78299010\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 74.9%  Avg loss: 1.75839235\n",
      "Test Error:\n",
      " Accuracy: 73.9%, Avg loss: 1.78185925\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 75.1%  Avg loss: 1.75724467\n",
      "Test Error:\n",
      " Accuracy: 74.1%, Avg loss: 1.78076378\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 75.3%  Avg loss: 1.75612060\n",
      "Test Error:\n",
      " Accuracy: 74.2%, Avg loss: 1.77961149\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 75.4%  Avg loss: 1.75501963\n",
      "Test Error:\n",
      " Accuracy: 74.2%, Avg loss: 1.77852846\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 75.5%  Avg loss: 1.75397220\n",
      "Test Error:\n",
      " Accuracy: 74.4%, Avg loss: 1.77765762\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error:\n",
      " Accuracy: 75.7%  Avg loss: 1.75301847\n",
      "Test Error:\n",
      " Accuracy: 74.5%, Avg loss: 1.77666481\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 75.8%  Avg loss: 1.75203320\n",
      "Test Error:\n",
      " Accuracy: 74.6%, Avg loss: 1.77566818\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 75.9%  Avg loss: 1.75102637\n",
      "Test Error:\n",
      " Accuracy: 74.6%, Avg loss: 1.77475454\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.0%  Avg loss: 1.75006896\n",
      "Test Error:\n",
      " Accuracy: 74.8%, Avg loss: 1.77388433\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.1%  Avg loss: 1.74911811\n",
      "Test Error:\n",
      " Accuracy: 74.9%, Avg loss: 1.77295929\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.2%  Avg loss: 1.74820907\n",
      "Test Error:\n",
      " Accuracy: 75.0%, Avg loss: 1.77210341\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.3%  Avg loss: 1.74731596\n",
      "Test Error:\n",
      " Accuracy: 75.1%, Avg loss: 1.77130014\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.3%  Avg loss: 1.74651580\n",
      "Test Error:\n",
      " Accuracy: 75.2%, Avg loss: 1.77050020\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.4%  Avg loss: 1.74571249\n",
      "Test Error:\n",
      " Accuracy: 75.3%, Avg loss: 1.76972550\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.4%  Avg loss: 1.74490086\n",
      "Test Error:\n",
      " Accuracy: 75.3%, Avg loss: 1.76887813\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.5%  Avg loss: 1.74408773\n",
      "Test Error:\n",
      " Accuracy: 75.4%, Avg loss: 1.76813654\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.6%  Avg loss: 1.74327451\n",
      "Test Error:\n",
      " Accuracy: 75.4%, Avg loss: 1.76745127\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.7%  Avg loss: 1.74250344\n",
      "Test Error:\n",
      " Accuracy: 75.5%, Avg loss: 1.76659931\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.8%  Avg loss: 1.74170654\n",
      "Test Error:\n",
      " Accuracy: 75.6%, Avg loss: 1.76592381\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.9%  Avg loss: 1.74099345\n",
      "Test Error:\n",
      " Accuracy: 75.7%, Avg loss: 1.76515812\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.0%  Avg loss: 1.74024270\n",
      "Test Error:\n",
      " Accuracy: 75.8%, Avg loss: 1.76446087\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.0%  Avg loss: 1.73952855\n",
      "Test Error:\n",
      " Accuracy: 75.9%, Avg loss: 1.76377953\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.1%  Avg loss: 1.73881758\n",
      "Test Error:\n",
      " Accuracy: 76.0%, Avg loss: 1.76311735\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.2%  Avg loss: 1.73815072\n",
      "Test Error:\n",
      " Accuracy: 76.0%, Avg loss: 1.76242539\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.3%  Avg loss: 1.73745783\n",
      "Test Error:\n",
      " Accuracy: 76.1%, Avg loss: 1.76181767\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.3%  Avg loss: 1.73680066\n",
      "Test Error:\n",
      " Accuracy: 76.2%, Avg loss: 1.76116531\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.4%  Avg loss: 1.73614542\n",
      "Test Error:\n",
      " Accuracy: 76.2%, Avg loss: 1.76056103\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.4%  Avg loss: 1.73551472\n",
      "Test Error:\n",
      " Accuracy: 76.3%, Avg loss: 1.76001239\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.5%  Avg loss: 1.73491614\n",
      "Test Error:\n",
      " Accuracy: 76.3%, Avg loss: 1.75945428\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.5%  Avg loss: 1.73434031\n",
      "Test Error:\n",
      " Accuracy: 76.4%, Avg loss: 1.75893429\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.6%  Avg loss: 1.73374854\n",
      "Test Error:\n",
      " Accuracy: 76.5%, Avg loss: 1.75840612\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.7%  Avg loss: 1.73317329\n",
      "Test Error:\n",
      " Accuracy: 76.6%, Avg loss: 1.75788374\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.7%  Avg loss: 1.73261616\n",
      "Test Error:\n",
      " Accuracy: 76.7%, Avg loss: 1.75735652\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.8%  Avg loss: 1.73203630\n",
      "Test Error:\n",
      " Accuracy: 76.7%, Avg loss: 1.75684376\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.8%  Avg loss: 1.73148154\n",
      "Test Error:\n",
      " Accuracy: 76.8%, Avg loss: 1.75634954\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.9%  Avg loss: 1.73094439\n",
      "Test Error:\n",
      " Accuracy: 76.8%, Avg loss: 1.75589263\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 77.9%  Avg loss: 1.73041106\n",
      "Test Error:\n",
      " Accuracy: 76.9%, Avg loss: 1.75539537\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.0%  Avg loss: 1.72989853\n",
      "Test Error:\n",
      " Accuracy: 77.0%, Avg loss: 1.75499606\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.1%  Avg loss: 1.72940824\n",
      "Test Error:\n",
      " Accuracy: 77.0%, Avg loss: 1.75455964\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.2%  Avg loss: 1.72890658\n",
      "Test Error:\n",
      " Accuracy: 77.0%, Avg loss: 1.75412720\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.2%  Avg loss: 1.72840690\n",
      "Test Error:\n",
      " Accuracy: 77.1%, Avg loss: 1.75368163\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.3%  Avg loss: 1.72791120\n",
      "Test Error:\n",
      " Accuracy: 77.1%, Avg loss: 1.75327037\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.3%  Avg loss: 1.72744715\n",
      "Test Error:\n",
      " Accuracy: 77.2%, Avg loss: 1.75287363\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.4%  Avg loss: 1.72697353\n",
      "Test Error:\n",
      " Accuracy: 77.2%, Avg loss: 1.75246291\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.5%  Avg loss: 1.72650840\n",
      "Test Error:\n",
      " Accuracy: 77.2%, Avg loss: 1.75205687\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.5%  Avg loss: 1.72604757\n",
      "Test Error:\n",
      " Accuracy: 77.3%, Avg loss: 1.75165921\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.5%  Avg loss: 1.72558333\n",
      "Test Error:\n",
      " Accuracy: 77.3%, Avg loss: 1.75128582\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 78.6%  Avg loss: 1.72514998\n",
      "Test Error:\n",
      " Accuracy: 77.3%, Avg loss: 1.75092975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(CFG.epoch):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(trainloader,model, loss_fn, optimizer)\n",
    "\n",
    "    test(testloader, model, loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
