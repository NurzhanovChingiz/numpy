{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590a051a",
   "metadata": {},
   "source": [
    "## Release notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94a5ba13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T09:38:31.045616Z",
     "start_time": "2023-09-23T09:38:31.042343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ml/Jupyter_root/123/CV using numpy'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e64c8",
   "metadata": {},
   "source": [
    "- v4 \n",
    "- Optimizer\n",
    "'''Update velocity using momentum and gradient\n",
    "velocity[i] = momentum * velocity[i] - lr * grad          \n",
    "Update parameter using new velocity\n",
    "param += velocity[i]\n",
    "Change to \n",
    "np.add(momentum * velocity[i], -lr * grad, out=velocity[i])\n",
    "- v3\n",
    "- add SGD with momentum\n",
    "- v2\n",
    "- Cleaned codes\n",
    "- vq\n",
    "-  Make all classes and compare them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0c12e",
   "metadata": {},
   "source": [
    "# Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2500151a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:47.564941Z",
     "start_time": "2023-09-21T07:51:46.879715Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b6fb6",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df9fa75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:47.567653Z",
     "start_time": "2023-09-21T07:51:47.565944Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a5bd4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:47.571837Z",
     "start_time": "2023-09-21T07:51:47.568581Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    batch = 256\n",
    "    shuffle = True\n",
    "    num_classes = 10\n",
    "    learning_rate = 0.01\n",
    "    epoch=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be6713",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1feaf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.407711Z",
     "start_time": "2023-09-21T07:51:47.572511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset using NumPy\n",
    "# Load train data\n",
    "train_data = np.loadtxt('train.csv', delimiter=',', skiprows=1)\n",
    "train_labels = train_data[:, 0]  # Labels 0 and 1\n",
    "train_data = train_data[:, 1:]   # Pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b3898",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c4c0e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.411673Z",
     "start_time": "2023-09-21T07:51:48.408515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 7., 6., 9.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4182934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.417914Z",
     "start_time": "2023-09-21T07:51:48.412411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels) # from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110f9876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.421491Z",
     "start_time": "2023-09-21T07:51:48.419156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee60447",
   "metadata": {},
   "source": [
    "## Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e54e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.487378Z",
     "start_time": "2023-09-21T07:51:48.422369Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = np.reshape(train_data, (train_data.shape[0],28,28,1)).copy()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e718eca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.490096Z",
     "start_time": "2023-09-21T07:51:48.488206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc11b338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.555401Z",
     "start_time": "2023-09-21T07:51:48.490726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4c819b9330>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1843b1ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.557943Z",
     "start_time": "2023-09-21T07:51:48.556045Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = np.einsum('bhwc ->bchw',train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd9b384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.561722Z",
     "start_time": "2023-09-21T07:51:48.559550Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50f4c2a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.575451Z",
     "start_time": "2023-09-21T07:51:48.562449Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, data, targets, batch_size, shuffle=True):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = len(self.data)\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for start_idx in range(0, self.num_samples, self.batch_size):\n",
    "            batch_indices = self.indices[start_idx:start_idx + self.batch_size]\n",
    "            batch_data = self.data[batch_indices]\n",
    "            batch_targets = self.targets[batch_indices]\n",
    "            yield batch_data, batch_targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e96ad079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.621528Z",
     "start_time": "2023-09-21T07:51:48.576814Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = CustomDataLoader(train_data[:10000].copy(), train_labels[:10000].copy(), CFG.batch, shuffle=CFG.shuffle)\n",
    "testloader = CustomDataLoader(train_data[10001:].copy(), train_labels[10001:].copy(), CFG.batch, shuffle=CFG.shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce03b3c",
   "metadata": {},
   "source": [
    "# NN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62b6aa",
   "metadata": {},
   "source": [
    "## Linaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eace8543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.625157Z",
     "start_time": "2023-09-21T07:51:48.622431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Linear layer class\n",
    "# Define your custom Linear class\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.initialize_parameters()  # Initialize weights and biases\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.gradW = np.dot(self.input.T, grad_output)\n",
    "        self.gradB = np.sum(grad_output, axis=0)\n",
    "        return np.dot(grad_output, self.W.T)\n",
    "    def initialize_parameters(self):\n",
    "        # Initialize weights and biases using random normal distribution\n",
    "        self.W = np.random.randn(self.in_features, self.out_features)\n",
    "        self.b = np.random.randn(self.out_features)\n",
    "        # Initialize gradients to zero\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradB = np.zeros_like(self.b)\n",
    "    def reset_parameters(self):\n",
    "        # Call initialize_parameters to reset weights and biases\n",
    "        self.initialize_parameters()\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fa778",
   "metadata": {},
   "source": [
    "## Linear class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2616e340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.652608Z",
     "start_time": "2023-09-21T07:51:48.626004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ReLU Forward Output:\n",
      "tensor([[-0.0157, -0.7415,  0.2387, -0.4539, -0.1759,  0.8366, -1.1666,  0.4954,\n",
      "          0.0784, -0.7994, -0.1749,  0.4661],\n",
      "        [ 0.0865,  0.2686, -0.3530, -0.5026,  0.1133, -0.2212, -0.6070, -0.9037,\n",
      "         -0.5375, -0.1062, -0.2841,  0.7298]], grad_fn=<AddmmBackward0>)\n",
      "Custom NumPy ReLU Forward Output:\n",
      "[[ 1.14471474 -0.62800154  4.08545362  2.05485693 -0.81900564  2.79300992\n",
      "   5.17957853 -1.04238972  2.74596107  1.7291386  -1.00781076  3.65433625]\n",
      " [ 1.78182189 -1.50972233  2.04841931  6.34874006 -1.44778071  1.74489852\n",
      "   0.33107905  1.33951084  0.82806944  0.87969348  4.69300491  4.92486044]]\n",
      "PyTorch ReLU Gradient (Backward Output):\n",
      "<bound method Tensor.backward of tensor([[-0.0157, -0.7415,  0.2387, -0.4539, -0.1759,  0.8366, -1.1666,  0.4954,\n",
      "          0.0784, -0.7994, -0.1749,  0.4661],\n",
      "        [ 0.0865,  0.2686, -0.3530, -0.5026,  0.1133, -0.2212, -0.6070, -0.9037,\n",
      "         -0.5375, -0.1062, -0.2841,  0.7298]], grad_fn=<AddmmBackward0>)>\n",
      "Custom NumPy ReLU Gradient (Backward Output):\n",
      "[[ 0.24502458  1.19793113 -0.63249539  0.98235602 -0.79615225 -0.88737027\n",
      "  -2.18535288  0.72065377 -1.80424757 -1.23186748]\n",
      " [ 3.0334825   1.09032269 -0.72121878 -1.54363975 -0.60752254  1.18746853\n",
      "  -1.17664647 -0.77843071 -0.27687469 -1.14739865]]\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "input_data = torch.Tensor(np.random.randn(2, 10))\n",
    "fc_torch = nn.Linear(in_features=10, out_features=12, bias=True)\n",
    "fc_numpy = Linear(in_features=10, out_features=12)\n",
    "# Create a PyTorch ReLU activation layer\n",
    "relu_torch = fc_torch\n",
    "\n",
    "# Create a custom ReLU activation layer\n",
    "relu_numpy = fc_numpy\n",
    "\n",
    "# Forward pass through PyTorch ReLU layer\n",
    "torch_out = relu_torch(input_data)\n",
    "\n",
    "# Forward pass through custom ReLU layer\n",
    "numpy_out = relu_numpy(input_data.detach().numpy())  # Convert PyTorch tensor to NumPy array\n",
    "\n",
    "# # Backward pass through custom ReLU layer (using the gradient output from the forward pass)\n",
    "grad_output = torch_out.detach().numpy()  # Convert PyTorch tensor to NumPy array\n",
    "custom_grad = relu_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch ReLU Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy ReLU Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "# # PyTorch's ReLU doesn't require backward pass for inference, so there's no gradient to print for PyTorch.\n",
    "print(\"PyTorch ReLU Gradient (Backward Output):\")\n",
    "print(torch_out.backward)\n",
    "print(\"Custom NumPy ReLU Gradient (Backward Output):\")\n",
    "print(custom_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91742288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.683112Z",
     "start_time": "2023-09-21T07:51:48.653347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Forward Output:\n",
      "tensor([[-0.9220,  0.3753,  0.5332,  0.7966, -0.0833, -0.7138,  0.4748, -0.2751,\n",
      "          0.5695, -0.3366,  0.4867, -0.2739],\n",
      "        [-0.7539, -0.7661, -0.3734, -0.6822, -0.0130, -0.2619, -0.0401,  0.0068,\n",
      "          0.9329,  0.4044, -0.3658,  0.4312]], grad_fn=<AddmmBackward0>)\n",
      "Custom NumPy Forward Output:\n",
      "[[-7.13156167 -6.8981223  -1.04939241 -6.08769731 -5.15242182  4.81681326\n",
      "  -1.69786389  2.78634205  1.06178507 -7.96418565 -2.24792321  0.53633638]\n",
      " [-2.32271291 -2.64110638  0.19314834 -4.71396282 -0.60491611  3.74209039\n",
      "  -5.20388654  1.64072579  3.62951277 -2.27910814 -1.39062624 -3.12419379]]\n",
      "PyTorch Weight Gradient:\n",
      "tensor([[-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874],\n",
      "        [-1.0515, -1.7819, -0.5366, -2.7084,  1.5946, -1.6576,  2.1384,  1.4776,\n",
      "          0.6355, -1.4874]])\n",
      "Custom NumPy Weight Gradient:\n",
      "[[-1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882\n",
      "  -1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882 -1.05146882]\n",
      " [-1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578\n",
      "  -1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578 -1.78192578]\n",
      " [-0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215\n",
      "  -0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215  -0.5366215 ]\n",
      " [-2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539\n",
      "  -2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539 -2.70842539]\n",
      " [ 1.59458314  1.59458314  1.59458314  1.59458314  1.59458314  1.59458314\n",
      "   1.59458314  1.59458314  1.59458314  1.59458314  1.59458314  1.59458314]\n",
      " [-1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259\n",
      "  -1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259 -1.65762259]\n",
      " [ 2.13842528  2.13842528  2.13842528  2.13842528  2.13842528  2.13842528\n",
      "   2.13842528  2.13842528  2.13842528  2.13842528  2.13842528  2.13842528]\n",
      " [ 1.47762419  1.47762419  1.47762419  1.47762419  1.47762419  1.47762419\n",
      "   1.47762419  1.47762419  1.47762419  1.47762419  1.47762419  1.47762419]\n",
      " [ 0.63552969  0.63552969  0.63552969  0.63552969  0.63552969  0.63552969\n",
      "   0.63552969  0.63552969  0.63552969  0.63552969  0.63552969  0.63552969]\n",
      " [-1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718\n",
      "  -1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718 -1.48741718]]\n",
      "PyTorch Bias Gradient:\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "Custom NumPy Bias Gradient:\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Create PyTorch and custom Linear layers\n",
    "fc_torch = nn.Linear(in_features=10, out_features=12, bias=True)\n",
    "fc_numpy = Linear(in_features=10, out_features=12)\n",
    "\n",
    "# Generate random input data\n",
    "input_data = np.random.randn(2, 10)\n",
    "\n",
    "# Forward pass through PyTorch layer\n",
    "torch_out = fc_torch(torch.Tensor(input_data))\n",
    "\n",
    "# Backward pass through PyTorch layer (this computes gradients)\n",
    "torch_out.backward(torch.ones_like(torch_out))\n",
    "\n",
    "# Forward pass through your custom Linear layer\n",
    "numpy_out = fc_numpy(input_data)\n",
    "\n",
    "# Backward pass through your custom Linear layer (using the gradient output from the forward pass)\n",
    "grad_output = np.ones_like(numpy_out)  # Gradient for the custom layer\n",
    "custom_grad = fc_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "print(\"PyTorch Weight Gradient:\")\n",
    "print(fc_torch.weight.grad)\n",
    "\n",
    "print(\"Custom NumPy Weight Gradient:\")\n",
    "print(fc_numpy.gradW)\n",
    "\n",
    "print(\"PyTorch Bias Gradient:\")\n",
    "print(fc_torch.bias.grad)\n",
    "\n",
    "print(\"Custom NumPy Bias Gradient:\")\n",
    "print(fc_numpy.gradB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71fcc8",
   "metadata": {},
   "source": [
    "## Relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8eb2e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.687148Z",
     "start_time": "2023-09-21T07:51:48.684456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define ReLU activation class\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x  # Store input for backpropagation\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input > 0)\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64ad04",
   "metadata": {},
   "source": [
    "## Relu class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0d9f350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.692299Z",
     "start_time": "2023-09-21T07:51:48.687894Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ReLU Forward Output:\n",
      "tensor([[0.0000, 0.8030, 0.0000, 0.6030, 0.7941, 0.0000, 0.0000, 0.1388, 1.1424,\n",
      "         0.0000],\n",
      "        [1.1378, 0.2551, 0.0000, 0.0000, 0.0000, 0.0000, 0.6068, 0.7978, 0.0000,\n",
      "         0.0477]])\n",
      "Custom NumPy ReLU Forward Output:\n",
      "[[0.         0.8029657  0.         0.6029578  0.7940915  0.\n",
      "  0.         0.13880739 1.1424047  0.        ]\n",
      " [1.1377505  0.25512746 0.         0.         0.         0.\n",
      "  0.60684925 0.79776895 0.         0.04768137]]\n",
      "PyTorch ReLU Gradient (Backward Output):\n",
      "<bound method Tensor.backward of tensor([[0.0000, 0.8030, 0.0000, 0.6030, 0.7941, 0.0000, 0.0000, 0.1388, 1.1424,\n",
      "         0.0000],\n",
      "        [1.1378, 0.2551, 0.0000, 0.0000, 0.0000, 0.0000, 0.6068, 0.7978, 0.0000,\n",
      "         0.0477]])>\n",
      "Custom NumPy ReLU Gradient (Backward Output):\n",
      "[[0.         0.8029657  0.         0.6029578  0.7940915  0.\n",
      "  0.         0.13880739 1.1424047  0.        ]\n",
      " [1.1377505  0.25512746 0.         0.         0.         0.\n",
      "  0.60684925 0.79776895 0.         0.04768137]]\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "input_data = torch.Tensor(np.random.randn(2, 10))\n",
    "\n",
    "# Create a PyTorch ReLU activation layer\n",
    "relu_torch = F.relu\n",
    "\n",
    "# Create a custom ReLU activation layer\n",
    "relu_numpy = ReLU()\n",
    "\n",
    "# Forward pass through PyTorch ReLU layer\n",
    "torch_out = relu_torch(input_data)\n",
    "\n",
    "# Forward pass through custom ReLU layer\n",
    "numpy_out = relu_numpy(input_data.detach().numpy())  # Convert PyTorch tensor to NumPy array\n",
    "\n",
    "# # Backward pass through custom ReLU layer (using the gradient output from the forward pass)\n",
    "grad_output = torch_out.detach().numpy()  # Convert PyTorch tensor to NumPy array\n",
    "custom_grad = relu_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch ReLU Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy ReLU Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "print(\"PyTorch ReLU Gradient (Backward Output):\")\n",
    "print(torch_out.backward)\n",
    "print(\"Custom NumPy ReLU Gradient (Backward Output):\")\n",
    "print(custom_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67191df",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8d667a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.696103Z",
     "start_time": "2023-09-21T07:51:48.693346Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         # Compute the exponential terms in a stable way.\n",
    "#         x_exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        \n",
    "#         # Compute the softmax output\n",
    "#         self.output = x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
    "        \n",
    "#         return self.output\n",
    "        x_max = np.amax(x, axis=1, keepdims=True)\n",
    "        exp_x_shifted = np.exp(x - x_max)\n",
    "        self.output = exp_x_shifted / np.sum(exp_x_shifted, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    def backward(self, d_output):\n",
    "        # Using broadcasting and vectorization to compute d_input in one shot\n",
    "        output_expanded = np.expand_dims(self.output, axis=2)  # Expand output dims for broadcasting\n",
    "        d_output_expanded = np.expand_dims(d_output, axis=2)  # Expand d_output dims for broadcasting\n",
    "        \n",
    "        jacobian_matrix = np.eye(self.output.shape[1]) - output_expanded * np.swapaxes(output_expanded, 1, 2)\n",
    "\n",
    "        d_input = np.einsum('ijk,ikj->ik', jacobian_matrix, d_output_expanded).squeeze()\n",
    "\n",
    "        return d_input\n",
    "#     def backward(self, d_output):\n",
    "#         s = d_output - self.output\n",
    "#         return d_output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91fb73",
   "metadata": {},
   "source": [
    "## Softmax class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "358051fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.702446Z",
     "start_time": "2023-09-21T07:51:48.697002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Softmax Forward Output:\n",
      "tensor([[0.0330, 0.0696, 0.4259, 0.1612, 0.0544, 0.0499, 0.0403, 0.0143, 0.0362,\n",
      "         0.1152],\n",
      "        [0.0346, 0.0663, 0.0287, 0.1926, 0.0246, 0.1588, 0.2456, 0.0635, 0.0442,\n",
      "         0.1412]])\n",
      "Custom NumPy Softmax Forward Output:\n",
      "[[0.03298177 0.06958842 0.425897   0.16116287 0.05441617 0.04987548\n",
      "  0.04029904 0.01431992 0.03622704 0.11523234]\n",
      " [0.03456811 0.0663466  0.02869632 0.19255742 0.02457472 0.15879717\n",
      "  0.24557206 0.06353392 0.04416914 0.14118454]]\n",
      "PyTorch Softmax Gradient (Backward Output):\n",
      "<bound method Tensor.backward of tensor([[0.0330, 0.0696, 0.4259, 0.1612, 0.0544, 0.0499, 0.0403, 0.0143, 0.0362,\n",
      "         0.1152],\n",
      "        [0.0346, 0.0663, 0.0287, 0.1926, 0.0246, 0.1588, 0.2456, 0.0635, 0.0442,\n",
      "         0.1412]])>\n",
      "Custom NumPy Softmax Gradient (Backward Output):\n",
      "[[0.03189397 0.06474587 0.24450872 0.13518938 0.05145505 0.04738791\n",
      "  0.03867503 0.01411486 0.03491464 0.10195384]\n",
      " [0.03337315 0.06194473 0.02787284 0.15547906 0.0239708  0.13358063\n",
      "  0.18526642 0.05949736 0.04221823 0.12125145]]\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "input_data = torch.Tensor(np.random.randn(2, 10))\n",
    "\n",
    "# Create a PyTorch Softmax activation layer\n",
    "softmax_torch = F.softmax\n",
    "\n",
    "# Create a custom Softmax activation layer\n",
    "softmax_numpy = Softmax()\n",
    "\n",
    "# Forward pass through PyTorch Softmax layer\n",
    "torch_out = softmax_torch(input_data, dim=1)\n",
    "\n",
    "# Forward pass through custom Softmax layer\n",
    "numpy_out = softmax_numpy(input_data.detach().numpy())  # Convert PyTorch tensor to NumPy array\n",
    "\n",
    "# # Backward pass through custom Softmax layer (using the gradient output from the forward pass)\n",
    "grad_output = torch_out.detach().numpy()  # Convert PyTorch tensor to NumPy array\n",
    "custom_grad = softmax_numpy.backward(grad_output)\n",
    "\n",
    "# Now, you can compare the outputs and gradients if needed\n",
    "print(\"PyTorch Softmax Forward Output:\")\n",
    "print(torch_out)\n",
    "\n",
    "print(\"Custom NumPy Softmax Forward Output:\")\n",
    "print(numpy_out)\n",
    "\n",
    "print(\"PyTorch Softmax Gradient (Backward Output):\")\n",
    "print(torch_out.backward)\n",
    "print(\"Custom NumPy Softmax Gradient (Backward Output):\")\n",
    "print(custom_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22d009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:38:36.364408Z",
     "start_time": "2023-09-20T04:38:36.362855Z"
    }
   },
   "source": [
    "## CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d44396cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.705816Z",
     "start_time": "2023-09-21T07:51:48.703178Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.softmax = Softmax()\n",
    "        self.epsilon = 1e-15\n",
    "    def forward(self, pred, y):\n",
    "        n = y.shape[0]\n",
    "        pred_softmax = self.softmax(pred)\n",
    "        log_pred = -np.log(pred_softmax[np.arange(n), y] + self.epsilon)\n",
    "        loss = np.sum(log_pred) / n\n",
    "\n",
    "        self.cache['pred_softmax'] = pred_softmax\n",
    "        self.cache['y'] = y\n",
    "        self.cache['n'] = n\n",
    "        self.cache['loss'] = loss\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        y = self.cache['y']\n",
    "        n = self.cache['n']\n",
    "        loss = self.cache['loss']\n",
    "        y = np.eye(CFG.num_classes)[y]\n",
    "        grad_input = self.softmax.output - y\n",
    "        grad_input /=n\n",
    "        return grad_input\n",
    "    \n",
    "    def __call__(self, pred, y):\n",
    "        return self.forward(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1111e",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss class compare numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c67ddbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.711958Z",
     "start_time": "2023-09-21T07:51:48.706441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients with respect to input predictions:\n",
      "tensor([[ 0.0856,  0.0660,  0.0160, -0.2429,  0.0137,  0.0032,  0.0099,  0.0271,\n",
      "          0.0119,  0.0095],\n",
      "        [ 0.0186,  0.0151,  0.0420,  0.0052,  0.0334,  0.0188, -0.2196,  0.0049,\n",
      "          0.0477,  0.0339],\n",
      "        [ 0.0886,  0.0072,  0.0093,  0.0028,  0.0073, -0.1802,  0.0223,  0.0080,\n",
      "          0.0190,  0.0156],\n",
      "        [ 0.0031, -0.2463,  0.0635,  0.0439,  0.0048,  0.0185,  0.0103,  0.0624,\n",
      "          0.0329,  0.0070]])\n",
      "Gradients with respect to input predictions:\n",
      "[[ 0.08563988  0.06598342  0.01602204 -0.24286835  0.01367588  0.00318298\n",
      "   0.00989491  0.02706582  0.01194149  0.00946193]\n",
      " [ 0.01864543  0.01505539  0.04195438  0.00522959  0.03336666  0.01882059\n",
      "  -0.21957186  0.00491351  0.04766207  0.03392425]\n",
      " [ 0.08861818  0.00723111  0.00926416  0.00279253  0.0072975  -0.18021398\n",
      "   0.02233584  0.00804276  0.01899586  0.01563605]\n",
      " [ 0.00311305 -0.24626936  0.06352987  0.04387507  0.0047598   0.01850181\n",
      "   0.01026798  0.0624005   0.0328616   0.00695966]]\n"
     ]
    }
   ],
   "source": [
    "pred = np.random.randn(4, 10)\n",
    "y = np.random.randint(0, 10, (4,))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize random predictions (logits) and labels for demonstration\n",
    "# Assume 4 samples and 10 classes\n",
    "\n",
    "pred_torch = torch.Tensor(pred.copy())\n",
    "pred_torch.requires_grad_()\n",
    "y_torch = torch.Tensor(y.copy()).long()\n",
    "# Initialize loss function\n",
    "loss_fn_torch = nn.CrossEntropyLoss()\n",
    "loss_fn_numpy = CrossEntropyLoss()\n",
    "# Forward pass to compute the loss\n",
    "loss_torch = loss_fn_torch(pred_torch, y_torch)\n",
    "loss_numpy = loss_fn_numpy(pred, y)\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss_torch.backward()\n",
    "\n",
    "# Print computed gradients for the input predictions\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(pred_torch.grad)\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(loss_fn_numpy.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c188ae6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.718355Z",
     "start_time": "2023-09-21T07:51:48.715240Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn_torch = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e8176a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.724189Z",
     "start_time": "2023-09-21T07:51:48.720696Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = np.random.randn(128,10)\n",
    "y = np.random.randint(0, 10, (128,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baceed0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.735147Z",
     "start_time": "2023-09-21T07:51:48.726603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients with respect to input predictions:\n",
      "tensor([[ 0.0012,  0.0010,  0.0010,  ...,  0.0003, -0.0075,  0.0016],\n",
      "        [ 0.0007,  0.0020,  0.0004,  ...,  0.0008,  0.0005,  0.0003],\n",
      "        [ 0.0007,  0.0020,  0.0006,  ...,  0.0002,  0.0005,  0.0018],\n",
      "        ...,\n",
      "        [ 0.0009,  0.0008,  0.0007,  ...,  0.0018,  0.0005,  0.0008],\n",
      "        [ 0.0003,  0.0013,  0.0016,  ...,  0.0013,  0.0005, -0.0063],\n",
      "        [ 0.0002,  0.0005,  0.0005,  ...,  0.0016,  0.0012,  0.0004]])\n",
      "Gradients with respect to input predictions:\n",
      "[[ 0.00116381  0.00099039  0.00099311 ...  0.00029668 -0.00745106\n",
      "   0.00157419]\n",
      " [ 0.00072999  0.00200638  0.00037059 ...  0.00083348  0.00051905\n",
      "   0.00025214]\n",
      " [ 0.00068369  0.00203043  0.00061834 ...  0.0001946   0.00045953\n",
      "   0.00180476]\n",
      " ...\n",
      " [ 0.00087967  0.00078998  0.00073329 ...  0.00177626  0.00051655\n",
      "   0.00084896]\n",
      " [ 0.00027092  0.00132709  0.0016218  ...  0.00134401  0.00045288\n",
      "  -0.00629786]\n",
      " [ 0.00023766  0.0004959   0.00052891 ...  0.00162981  0.001194\n",
      "   0.00043709]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pred_torch = torch.Tensor(pred.copy())\n",
    "pred_torch.requires_grad_()\n",
    "y_torch = torch.Tensor(y.copy()).long()\n",
    "# Initialize loss function\n",
    "loss_fn_torch = nn.CrossEntropyLoss()\n",
    "loss_fn_numpy = CrossEntropyLoss()\n",
    "# Forward pass to compute the loss\n",
    "loss_torch = loss_fn_torch(pred_torch, y_torch)\n",
    "loss_numpy = loss_fn_numpy(pred, y)\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss_torch.backward()\n",
    "\n",
    "# Print computed gradients for the input predictions\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(pred_torch.grad)\n",
    "print(\"Gradients with respect to input predictions:\")\n",
    "print(loss_fn_numpy.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fd028",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87842120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.743699Z",
     "start_time": "2023-09-21T07:51:48.736904Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_size, out_dim):\n",
    "        self.fc1 = Linear(in_features = input_size, out_features = 32)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(in_features=32, out_features=64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc3 = Linear(in_features=64, out_features=out_dim)\n",
    "        self.relu3 = ReLU()\n",
    "        self.soft = Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.soft(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output = self.soft.backward(grad_output)\n",
    "        grad_output = self.relu3.backward(grad_output)\n",
    "        grad_output = self.fc3.backward(grad_output)\n",
    "        grad_output = self.relu2.backward(grad_output)\n",
    "        grad_output = self.fc2.backward(grad_output)\n",
    "        grad_output = self.relu1.backward(grad_output)\n",
    "        grad_output = self.fc1.backward(grad_output)\n",
    "        \n",
    "    def params(self):\n",
    "        # Collect parameters and their gradients in a list of tuples\n",
    "        parameters = [\n",
    "            (self.fc1.W, self.fc1.gradW),\n",
    "            (self.fc1.b, self.fc1.gradB),\n",
    "            (self.fc2.W, self.fc2.gradW),\n",
    "            (self.fc2.b, self.fc2.gradB),\n",
    "            (self.fc3.W, self.fc3.gradW),\n",
    "            (self.fc3.b, self.fc3.gradB)\n",
    "        ]\n",
    "        return parameters\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dda7895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.749219Z",
     "start_time": "2023-09-21T07:51:48.745133Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SimpleNN(28*28,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66af13",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "291f6d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.754133Z",
     "start_time": "2023-09-21T07:51:48.750541Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    @classmethod\n",
    "    def step(cls, parameters, lr, momentum = 0.9,velocity=None, with_momentum = False):\n",
    "        if with_momentum and velocity is None:\n",
    "            # Init velocity\n",
    "            velocity = [np.zeros_like(param) for param, _ in parameters]\n",
    "            \n",
    "        if with_momentum:\n",
    "            for i, (param, grad) in enumerate(parameters):\n",
    "                # Update velocity using momentum and gradient\n",
    "                np.add(momentum * velocity[i], -lr * grad, out=velocity[i])\n",
    "\n",
    "            return velocity\n",
    "        else: \n",
    "            for param, grad in parameters:\n",
    "                param -= lr * grad\n",
    "            return None\n",
    "                \n",
    "    @classmethod        \n",
    "    def zero_grad(cls, parameters):\n",
    "        for _, grad in parameters:\n",
    "            grad.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a29e4aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.758183Z",
     "start_time": "2023-09-21T07:51:48.755681Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77800a9c",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61e04c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.761886Z",
     "start_time": "2023-09-21T07:51:48.758766Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa48ee5",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dccc7c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.765859Z",
     "start_time": "2023-09-21T07:51:48.762520Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader)\n",
    "    num_batches =(len(dataloader) + CFG.batch - 1) // CFG.batch\n",
    "    \n",
    "    train_loss, correct = 0, 0\n",
    "    \n",
    "    for b, (X, y) in enumerate(dataloader):\n",
    "#         y = y.astype(int)\n",
    "        y = y.astype(np.int32)\n",
    "        pred = model(X)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "        grad = loss_fn.backward()\n",
    "\n",
    "        model.backward(grad)\n",
    "        # Update weights\n",
    "        optimizer.step(model.params(), CFG.learning_rate,momentum=0.9, with_momentum=False)  # To update parameters\n",
    "        optimizer.zero_grad(model.params())  # To reset gradients to zero\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum()       \n",
    "    train_loss /= num_batches\n",
    "    accuracy = (100 * correct) / size\n",
    "    print(f\"Train error:\\n Accuracy: {accuracy:.1f}%  Avg loss: {train_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09b57fb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T10:52:12.115394Z",
     "start_time": "2023-09-21T10:52:12.066713Z"
    }
   },
   "outputs": [],
   "source": [
    "size = len(trainloader)\n",
    "num_batches =(len(trainloader) + CFG.batch - 1) // CFG.batch\n",
    "    \n",
    "train_loss, correct = 0, 0\n",
    "    \n",
    "for b, (X, y) in enumerate(trainloader):\n",
    "#         y = y.astype(int)\n",
    "    y = y.astype(np.int32)\n",
    "    pred = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e66757f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T10:51:37.597984Z",
     "start_time": "2023-09-21T10:51:37.596275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 32)\n"
     ]
    }
   ],
   "source": [
    "for par, grad in model.params():\n",
    "    print(par.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8043e00",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f84b3a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:51:48.770141Z",
     "start_time": "2023-09-21T07:51:48.766579Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for b, (X, y) in enumerate(testloader):\n",
    "        y = y.astype(int)\n",
    "#         y = y.astype(np.int32)\n",
    "#         y = np.eye(CFG.num_classes)[y]\n",
    "        pred = model(X)\n",
    "        pred = pred.astype(int)\n",
    "#         pred = np.eye(CFG.num_classes)[pred]\n",
    "        test_loss += loss_fn(pred, y)\n",
    "        total += len(y)\n",
    "        correct += (pred.argmax(1) == y).sum()\n",
    "    loss = test_loss / b\n",
    "    correct = 100.*correct/total\n",
    "    print(f\"Test Error:\\n Accuracy: {(correct):>.1f}%, Avg loss: {(loss):>.8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415884e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9484c202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T07:52:13.975387Z",
     "start_time": "2023-09-21T07:51:48.770901Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 22.4%  Avg loss: 2.23535051\n",
      "Test Error:\n",
      " Accuracy: 25.1%, Avg loss: 2.20497736\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 38.9%  Avg loss: 2.06844931\n",
      "Test Error:\n",
      " Accuracy: 40.0%, Avg loss: 2.06394306\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 51.1%  Avg loss: 1.94695979\n",
      "Test Error:\n",
      " Accuracy: 51.8%, Avg loss: 1.95240519\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 60.5%  Avg loss: 1.85269175\n",
      "Test Error:\n",
      " Accuracy: 62.9%, Avg loss: 1.84414126\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 71.4%  Avg loss: 1.74446918\n",
      "Test Error:\n",
      " Accuracy: 70.3%, Avg loss: 1.76452654\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 76.6%  Avg loss: 1.69403172\n",
      "Test Error:\n",
      " Accuracy: 72.7%, Avg loss: 1.74036716\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 79.1%  Avg loss: 1.66793994\n",
      "Test Error:\n",
      " Accuracy: 77.0%, Avg loss: 1.69858460\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 80.8%  Avg loss: 1.65154538\n",
      "Test Error:\n",
      " Accuracy: 78.2%, Avg loss: 1.68620959\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 82.1%  Avg loss: 1.63890040\n",
      "Test Error:\n",
      " Accuracy: 79.1%, Avg loss: 1.67755657\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 82.9%  Avg loss: 1.63225682\n",
      "Test Error:\n",
      " Accuracy: 80.1%, Avg loss: 1.66770064\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 83.7%  Avg loss: 1.62375669\n",
      "Test Error:\n",
      " Accuracy: 81.1%, Avg loss: 1.65847277\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 84.4%  Avg loss: 1.61801211\n",
      "Test Error:\n",
      " Accuracy: 81.4%, Avg loss: 1.65567673\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 84.8%  Avg loss: 1.61221408\n",
      "Test Error:\n",
      " Accuracy: 82.4%, Avg loss: 1.64647911\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 85.2%  Avg loss: 1.60866819\n",
      "Test Error:\n",
      " Accuracy: 82.9%, Avg loss: 1.64106561\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 85.6%  Avg loss: 1.60452535\n",
      "Test Error:\n",
      " Accuracy: 83.4%, Avg loss: 1.63690137\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 86.0%  Avg loss: 1.60077861\n",
      "Test Error:\n",
      " Accuracy: 83.6%, Avg loss: 1.63447243\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 86.3%  Avg loss: 1.59775762\n",
      "Test Error:\n",
      " Accuracy: 83.8%, Avg loss: 1.63325825\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 86.6%  Avg loss: 1.59599231\n",
      "Test Error:\n",
      " Accuracy: 84.2%, Avg loss: 1.62868755\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 87.0%  Avg loss: 1.59116446\n",
      "Test Error:\n",
      " Accuracy: 84.6%, Avg loss: 1.62538899\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 87.0%  Avg loss: 1.59099804\n",
      "Test Error:\n",
      " Accuracy: 84.7%, Avg loss: 1.62398061\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 87.4%  Avg loss: 1.58689603\n",
      "Test Error:\n",
      " Accuracy: 84.9%, Avg loss: 1.62232945\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 87.5%  Avg loss: 1.58558231\n",
      "Test Error:\n",
      " Accuracy: 85.2%, Avg loss: 1.61966675\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 87.9%  Avg loss: 1.58245708\n",
      "Test Error:\n",
      " Accuracy: 85.0%, Avg loss: 1.62141338\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 88.0%  Avg loss: 1.58263715\n",
      "Test Error:\n",
      " Accuracy: 84.6%, Avg loss: 1.62543415\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 88.2%  Avg loss: 1.57966464\n",
      "Test Error:\n",
      " Accuracy: 85.5%, Avg loss: 1.61701465\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 88.3%  Avg loss: 1.57829092\n",
      "Test Error:\n",
      " Accuracy: 85.2%, Avg loss: 1.61950873\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 88.6%  Avg loss: 1.57565093\n",
      "Test Error:\n",
      " Accuracy: 85.7%, Avg loss: 1.61479369\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 88.8%  Avg loss: 1.57343604\n",
      "Test Error:\n",
      " Accuracy: 85.7%, Avg loss: 1.61533989\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 88.9%  Avg loss: 1.57203497\n",
      "Test Error:\n",
      " Accuracy: 86.0%, Avg loss: 1.61147421\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 89.2%  Avg loss: 1.56812692\n",
      "Test Error:\n",
      " Accuracy: 86.8%, Avg loss: 1.60327509\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 89.3%  Avg loss: 1.56831696\n",
      "Test Error:\n",
      " Accuracy: 86.3%, Avg loss: 1.60898591\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 89.4%  Avg loss: 1.56641006\n",
      "Test Error:\n",
      " Accuracy: 87.0%, Avg loss: 1.60221922\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 89.5%  Avg loss: 1.56606908\n",
      "Test Error:\n",
      " Accuracy: 86.5%, Avg loss: 1.60724321\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 89.7%  Avg loss: 1.56316571\n",
      "Test Error:\n",
      " Accuracy: 87.1%, Avg loss: 1.60067143\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.0%  Avg loss: 1.56184885\n",
      "Test Error:\n",
      " Accuracy: 86.8%, Avg loss: 1.60367385\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 89.8%  Avg loss: 1.56172520\n",
      "Test Error:\n",
      " Accuracy: 87.3%, Avg loss: 1.59877235\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.1%  Avg loss: 1.55955493\n",
      "Test Error:\n",
      " Accuracy: 87.4%, Avg loss: 1.59751162\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.3%  Avg loss: 1.55715683\n",
      "Test Error:\n",
      " Accuracy: 87.6%, Avg loss: 1.59566693\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.4%  Avg loss: 1.55777234\n",
      "Test Error:\n",
      " Accuracy: 87.1%, Avg loss: 1.60077902\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.4%  Avg loss: 1.55596119\n",
      "Test Error:\n",
      " Accuracy: 87.6%, Avg loss: 1.59559722\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.6%  Avg loss: 1.55371411\n",
      "Test Error:\n",
      " Accuracy: 87.7%, Avg loss: 1.59442183\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.8%  Avg loss: 1.55222019\n",
      "Test Error:\n",
      " Accuracy: 87.9%, Avg loss: 1.59200002\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.9%  Avg loss: 1.55157494\n",
      "Test Error:\n",
      " Accuracy: 87.9%, Avg loss: 1.59186895\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.8%  Avg loss: 1.55307539\n",
      "Test Error:\n",
      " Accuracy: 87.4%, Avg loss: 1.59832449\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 90.8%  Avg loss: 1.55204470\n",
      "Test Error:\n",
      " Accuracy: 88.2%, Avg loss: 1.58994835\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.1%  Avg loss: 1.54883110\n",
      "Test Error:\n",
      " Accuracy: 88.3%, Avg loss: 1.58911448\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.2%  Avg loss: 1.54823865\n",
      "Test Error:\n",
      " Accuracy: 88.3%, Avg loss: 1.58879536\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.2%  Avg loss: 1.54851950\n",
      "Test Error:\n",
      " Accuracy: 88.4%, Avg loss: 1.58759380\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.3%  Avg loss: 1.54721938\n",
      "Test Error:\n",
      " Accuracy: 88.5%, Avg loss: 1.58668332\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.5%  Avg loss: 1.54559784\n",
      "Test Error:\n",
      " Accuracy: 88.4%, Avg loss: 1.58710231\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.5%  Avg loss: 1.54479975\n",
      "Test Error:\n",
      " Accuracy: 88.6%, Avg loss: 1.58524328\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.5%  Avg loss: 1.54541533\n",
      "Test Error:\n",
      " Accuracy: 88.7%, Avg loss: 1.58452607\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.6%  Avg loss: 1.54581023\n",
      "Test Error:\n",
      " Accuracy: 87.8%, Avg loss: 1.59383431\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.5%  Avg loss: 1.54471621\n",
      "Test Error:\n",
      " Accuracy: 88.7%, Avg loss: 1.58398468\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.6%  Avg loss: 1.54392499\n",
      "Test Error:\n",
      " Accuracy: 88.7%, Avg loss: 1.58423508\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.7%  Avg loss: 1.54291900\n",
      "Test Error:\n",
      " Accuracy: 88.8%, Avg loss: 1.58313682\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.8%  Avg loss: 1.54176819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      " Accuracy: 88.8%, Avg loss: 1.58277936\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.9%  Avg loss: 1.54110727\n",
      "Test Error:\n",
      " Accuracy: 88.8%, Avg loss: 1.58271262\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.8%  Avg loss: 1.54177290\n",
      "Test Error:\n",
      " Accuracy: 88.8%, Avg loss: 1.58302424\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.9%  Avg loss: 1.54074002\n",
      "Test Error:\n",
      " Accuracy: 88.7%, Avg loss: 1.58357782\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.0%  Avg loss: 1.54032147\n",
      "Test Error:\n",
      " Accuracy: 89.0%, Avg loss: 1.58162839\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.0%  Avg loss: 1.54002595\n",
      "Test Error:\n",
      " Accuracy: 89.0%, Avg loss: 1.58106079\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.0%  Avg loss: 1.53974735\n",
      "Test Error:\n",
      " Accuracy: 88.8%, Avg loss: 1.58303376\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.0%  Avg loss: 1.54077358\n",
      "Test Error:\n",
      " Accuracy: 87.9%, Avg loss: 1.59257872\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 91.9%  Avg loss: 1.54048142\n",
      "Test Error:\n",
      " Accuracy: 89.0%, Avg loss: 1.58119250\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.0%  Avg loss: 1.53908992\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.58023018\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.3%  Avg loss: 1.53708095\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.58022438\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.2%  Avg loss: 1.53734572\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.58069250\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.2%  Avg loss: 1.53747882\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57893615\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.3%  Avg loss: 1.53605125\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57927378\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.3%  Avg loss: 1.53652428\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57928123\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.4%  Avg loss: 1.53496261\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57945887\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.2%  Avg loss: 1.53681387\n",
      "Test Error:\n",
      " Accuracy: 89.3%, Avg loss: 1.57847939\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.4%  Avg loss: 1.53561732\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57872557\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.4%  Avg loss: 1.53545756\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57884270\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.5%  Avg loss: 1.53493302\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57874473\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.5%  Avg loss: 1.53485587\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.57955200\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.6%  Avg loss: 1.53345112\n",
      "Test Error:\n",
      " Accuracy: 89.4%, Avg loss: 1.57704552\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.7%  Avg loss: 1.53245987\n",
      "Test Error:\n",
      " Accuracy: 89.3%, Avg loss: 1.57812011\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.6%  Avg loss: 1.53312501\n",
      "Test Error:\n",
      " Accuracy: 89.3%, Avg loss: 1.57769667\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.5%  Avg loss: 1.53407673\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57890904\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.7%  Avg loss: 1.53279862\n",
      "Test Error:\n",
      " Accuracy: 89.3%, Avg loss: 1.57807935\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.7%  Avg loss: 1.53269737\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57882494\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.7%  Avg loss: 1.53237387\n",
      "Test Error:\n",
      " Accuracy: 89.3%, Avg loss: 1.57760908\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.7%  Avg loss: 1.53190429\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.57902396\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.8%  Avg loss: 1.53139817\n",
      "Test Error:\n",
      " Accuracy: 89.3%, Avg loss: 1.57803177\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.8%  Avg loss: 1.53137905\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57859099\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.9%  Avg loss: 1.52971464\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57887912\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.9%  Avg loss: 1.53008482\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57884613\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.0%  Avg loss: 1.52928249\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.57889029\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 92.9%  Avg loss: 1.53024954\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57849872\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.0%  Avg loss: 1.52933384\n",
      "Test Error:\n",
      " Accuracy: 89.3%, Avg loss: 1.57754601\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.0%  Avg loss: 1.52868138\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.57902520\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.0%  Avg loss: 1.52916293\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57841553\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.0%  Avg loss: 1.52865656\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57848312\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.1%  Avg loss: 1.52824906\n",
      "Test Error:\n",
      " Accuracy: 89.0%, Avg loss: 1.57969684\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.0%  Avg loss: 1.52862379\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57820481\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.1%  Avg loss: 1.52796338\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.57883426\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.0%  Avg loss: 1.52876561\n",
      "Test Error:\n",
      " Accuracy: 89.1%, Avg loss: 1.57856246\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train error:\n",
      " Accuracy: 93.1%  Avg loss: 1.52849881\n",
      "Test Error:\n",
      " Accuracy: 89.2%, Avg loss: 1.57834325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(CFG.epoch):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(trainloader,model, loss_fn, optimizer)\n",
    "\n",
    "    test(testloader, model, loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
